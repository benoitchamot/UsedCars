{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert CSV to parquet files\n",
    "While the CSV files can be used to load data into PostgreSQL (with pgAdmin) and MySQL (with Workbench), we use this notebook and Spark to load the CSV and save them as parquet files which can then be used on cloud services such as Databricks or AWS SageMaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# Initialise findspark \n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "# Start a Spark session\n",
    "spark = SparkSession.builder.appName(\"Used Cars ETL\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv_to_parquet(filename):\n",
    "    # Import files\n",
    "    csv_path = f'cleaned_csv/{filename}.csv'\n",
    "    spark.sparkContext.addFile(csv_path)\n",
    "\n",
    "    # Add CSV to DataFrame\n",
    "    print(\"Loading file...\", end='\\r')\n",
    "    df = spark.read.csv(SparkFiles.get(f\"{filename}.csv\"), sep=\",\", header=True)\n",
    "    print(\"Loading file... Done.\")\n",
    "    \n",
    "    # Export DataFrame to parquet file\n",
    "    print(\"Write parquet files...\", end='\\r')\n",
    "    df.write.parquet(f'parquet/{filename}', mode='overwrite')\n",
    "    print(\"Write parquet files... Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Done.\n",
      "Write parquet files... Done.\n"
     ]
    }
   ],
   "source": [
    "write_csv_to_parquet('location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Done.\n",
      "Write parquet files... Done.\n"
     ]
    }
   ],
   "source": [
    "write_csv_to_parquet('listing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Done.\n",
      "Write parquet files... Done.\n"
     ]
    }
   ],
   "source": [
    "write_csv_to_parquet('usage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Done.\n",
      "Write parquet files... Done.\n"
     ]
    }
   ],
   "source": [
    "write_csv_to_parquet('vehicle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
